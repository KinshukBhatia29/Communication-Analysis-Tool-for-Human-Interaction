{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T18:55:12.577413Z",
     "iopub.status.busy": "2025-04-07T18:55:12.577108Z",
     "iopub.status.idle": "2025-04-07T18:55:32.257677Z",
     "shell.execute_reply": "2025-04-07T18:55:32.256779Z",
     "shell.execute_reply.started": "2025-04-07T18:55:12.577366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.9.0)\n",
      "Collecting triton>=2.0.0 (from openai-whisper)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803405 sha256=0e0a306cbf4707c8cad21406ab9dec0bf62ac987ab84eb208332603f0b28bc8d\n",
      "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: triton, ffmpeg-python, openai-whisper\n",
      "Successfully installed ffmpeg-python-0.2.0 openai-whisper-20240930 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg-python pandas nltk seaborn matplotlib openai-whisper pandas\n",
    "# ffmpeg-python: Used to extract audio from video files and convert them to mono 16kHz WAV format.\n",
    "# pandas: Used for data manipulation, creating dataframes, and exporting the final transcription and analysis results as a CSV.\n",
    "# nltk: Used for natural language processing tasks, specifically sentiment analysis using the VADER model.\n",
    "# seaborn: Used for visualizing data, such as sentiment distribution.\n",
    "# matplotlib: Used for plotting histograms and other visual representations of data.\n",
    "# openai-whisper: A pre-trained model used for automatic speech recognition (ASR) to generate transcriptions from audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:55:55.755252Z",
     "iopub.status.busy": "2025-04-07T18:55:55.754941Z",
     "iopub.status.idle": "2025-04-07T18:56:17.350848Z",
     "shell.execute_reply": "2025-04-07T18:56:17.349770Z",
     "shell.execute_reply.started": "2025-04-07T18:55:55.755224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyannote.audio[all]\n",
      "  Downloading pyannote.audio-3.3.2-py2.py3-none-any.whl.metadata (11 kB)\n",
      "\u001b[33mWARNING: pyannote-audio 3.3.2 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting asteroid-filterbanks>=0.4 (from pyannote.audio[all])\n",
      "  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (0.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (0.29.0)\n",
      "Collecting lightning>=2.0.1 (from pyannote.audio[all])\n",
      "  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (2.3.0)\n",
      "Collecting pyannote.core>=5.0.0 (from pyannote.audio[all])\n",
      "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pyannote.database>=5.0.1 (from pyannote.audio[all])\n",
      "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyannote.metrics>=3.2 (from pyannote.audio[all])\n",
      "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyannote.pipeline>=3.0.1 (from pyannote.audio[all])\n",
      "  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\n",
      "Collecting pytorch-metric-learning>=2.1.0 (from pyannote.audio[all])\n",
      "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (13.9.4)\n",
      "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (3.0.4)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (0.12.1)\n",
      "Collecting speechbrain>=1.0.0 (from pyannote.audio[all])\n",
      "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tensorboardX>=2.6 (from pyannote.audio[all])\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (2.5.1+cu121)\n",
      "Collecting torch-audiomentations>=0.11.0 (from pyannote.audio[all])\n",
      "  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torchaudio>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio[all]) (1.6.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio[all]) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio[all]) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio[all]) (4.67.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio[all]) (0.12.0)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio[all]) (2.5.0.post0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio[all]) (4.9.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio[all]) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio[all]) (1.13.1)\n",
      "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio[all]) (2.2.3)\n",
      "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio[all]) (0.15.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio[all]) (1.2.2)\n",
      "Collecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio[all])\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio[all]) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio[all]) (3.7.5)\n",
      "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio[all]) (1.13.1)\n",
      "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio[all]) (4.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio[all]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio[all]) (2.19.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio[all]) (1.17.1)\n",
      "Collecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio[all])\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio[all]) (1.4.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio[all]) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.6->pyannote.audio[all]) (3.20.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio[all]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio[all]) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio[all]) (1.3.0)\n",
      "Collecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio[all])\n",
      "  Downloading julius-0.2.7.tar.gz (59 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio[all])\n",
      "  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio[all]) (2.22)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (3.11.12)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio[all]) (75.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio[all]) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2.4.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (1.14.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (2.0.36)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio[all]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio[all]) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio[all]) (3.5.0)\n",
      "Collecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio[all])\n",
      "  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio[all]) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio[all]) (1.5.4)\n",
      "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio[all])\n",
      "  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio[all]) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio[all]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio[all]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio[all]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio[all]) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio[all]) (1.18.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (1.3.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio[all]) (1.17.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio[all])\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio[all]) (3.1.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->asteroid-filterbanks>=0.4->pyannote.audio[all]) (2024.2.0)\n",
      "Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\n",
      "Downloading lightning-2.5.1-py3-none-any.whl (818 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\n",
      "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m120.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m32.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.audio-3.3.2-py2.py3-none-any.whl (898 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\n",
      "Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading primePy-1.3-py3-none-any.whl (4.0 kB)\n",
      "Downloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt, julius\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=c4bb6650d93b877c97fd097f8f678f84cd845c743957951645b577e3834c2b7b\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21869 sha256=6e5cb21bb69ee031330cd1bd1fb0168933d7307c80bdb06c53f7630425917a7a\n",
      "  Stored in directory: /root/.cache/pip/wheels/b9/b2/05/f883527ffcb7f2ead5438a2c23439aa0c881eaa9a4c80256f4\n",
      "Successfully built docopt julius\n",
      "Installing collected packages: primePy, docopt, ruamel.yaml.clib, ruamel.yaml, julius, hyperpyyaml, torch-pitch-shift, torch-audiomentations, pyannote.core, pyannote.database, tensorboardX, speechbrain, pytorch-metric-learning, pyannote.pipeline, pyannote.metrics, lightning, asteroid-filterbanks, pyannote.audio\n",
      "Successfully installed asteroid-filterbanks-0.4.0 docopt-0.6.2 hyperpyyaml-1.2.2 julius-0.2.7 lightning-2.5.1 primePy-1.3 pyannote.audio-3.3.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-metric-learning-2.8.1 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 speechbrain-1.0.3 tensorboardX-2.6.2.2 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2.4.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3->librosa) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"pyannote.audio[all]\"\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install librosa\n",
    "# pyannote.audio[all]: Used for speaker diarization to identify and segment speakers within the audio locally.\n",
    "# torch: Core library required to run Whisper and pyannote.audio models.\n",
    "# torchvision: Included as part of the PyTorch ecosystem (not directly used here but required in some environments).\n",
    "# torchaudio: Handles audio processing within the PyTorch ecosystem.\n",
    "# librosa: Audio analysis library; helpful for processing and analyzing audio signals (optional but useful for extra features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step-by-Step Workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Data Ingestion**\n",
    "#### Loads all video files, extracts audio, and converts it to mono 16kHz WAV format for further processing.\n",
    "\n",
    "## **Step 2: Transcription**\n",
    "#### Uses Whisper (offline model) to transcribe spoken words in the audio into text along with start and end timestamps.\n",
    "\n",
    "## **Step 3: Speaker Diarization**\n",
    "#### Uses a pre-trained local model (like pyannote) to assign speaker labels to different segments of the audio, distinguishing between different voices.\n",
    "\n",
    "## **Step 4: Time Bucketing**\n",
    "#### Segments each transcription line into 5-second intervals based on start time for better temporal analysis and aggregation.\n",
    "\n",
    "## **Step 5: Sentiment Analysis**\n",
    "#### Analyzes the sentiment of each transcribed text segment using NLTK's VADER model and labels them as positive, negative, or neutral.\n",
    "\n",
    "## **Step 6: Named Entity Recognition (NER)**\n",
    "#### Applies a Hugging Face NER model to detect and extract entities (like names, places, orgs) from each text segment.\n",
    "\n",
    "## **Step 7: Export to CSV**\n",
    "#### Combines all processed results (transcription, speakers, timestamps, sentiment, entities) into a single structured CSV for analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:57:51.250950Z",
     "iopub.status.busy": "2025-04-07T18:57:51.250666Z",
     "iopub.status.idle": "2025-04-07T18:57:51.292491Z",
     "shell.execute_reply": "2025-04-07T18:57:51.291786Z",
     "shell.execute_reply.started": "2025-04-07T18:57:51.250929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:58:51.328906Z",
     "iopub.status.busy": "2025-04-07T18:58:51.328605Z",
     "iopub.status.idle": "2025-04-07T18:58:51.471693Z",
     "shell.execute_reply": "2025-04-07T18:58:51.470872Z",
     "shell.execute_reply.started": "2025-04-07T18:58:51.328886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')  # Required for sentiment analysis\n",
    "nltk.download('punkt')          # Tokenizer for text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Data Ingestion (Video to Audio Conversion)** \n",
    "#### * Load video files from the input directory.\n",
    "#### * Extract audio from each video using ffmpeg.\n",
    "#### * Convert audio to mono and 16kHz .wav format.\n",
    "#### * Store the resulting audio files in an output folder (e.g., ./output_audio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:59:34.768968Z",
     "iopub.status.busy": "2025-04-07T18:59:34.768643Z",
     "iopub.status.idle": "2025-04-07T18:59:39.323589Z",
     "shell.execute_reply": "2025-04-07T18:59:39.322825Z",
     "shell.execute_reply.started": "2025-04-07T18:59:34.768941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed: Experimenter_CREW_999_1_All_1731617801.mp4 â†’ ./output_audio/Experimenter_CREW_999_1_All_1731617801.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def extract_audio(video_path, output_audio_path):\n",
    "    \"\"\"\n",
    "    Extracts audio from a single video and saves it as mono 16kHz WAV.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to the input video file.\n",
    "        output_audio_path (str): Path where the output audio will be saved.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved audio file.\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\",\n",
    "        \"-ac\", \"1\", \"-ar\", \"16000\",  # Mono & 16kHz\n",
    "        output_audio_path, \"-y\"\n",
    "    ]\n",
    "    subprocess.run(cmd, capture_output=True, text=True)\n",
    "    return output_audio_path\n",
    "\n",
    "def process_all_videos(input_video_dir):\n",
    "    \"\"\"\n",
    "    Processes all video files in the input directory,\n",
    "    extracts audio, and saves them to ./output_audio/.\n",
    "\n",
    "    Parameters:\n",
    "        input_video_dir (str): Directory containing video files.\n",
    "    \"\"\"\n",
    "    output_audio_dir = \"./output_audio\"\n",
    "    os.makedirs(output_audio_dir, exist_ok=True)\n",
    "\n",
    "    video_extensions = (\".mp4\", \".mov\", \".avi\", \".mkv\")\n",
    "\n",
    "    for filename in os.listdir(input_video_dir):\n",
    "        if filename.lower().endswith(video_extensions):\n",
    "            video_path = os.path.join(input_video_dir, filename)\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            output_audio_path = os.path.join(output_audio_dir, f\"{base_name}.wav\")\n",
    "\n",
    "            extract_audio(video_path, output_audio_path)\n",
    "            print(f\"âœ… Processed: {filename} â†’ {output_audio_path}\")\n",
    "\n",
    "# ðŸŸ¡ Call this function with your actual input folder containing videos\n",
    "input_video_directory = \"Your input video directory\"\n",
    "process_all_videos(input_video_directory)  # Change path as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Transcription (Using Whisper)**\n",
    "#### * Load a pre-trained Whisper model (base, small, etc.).\n",
    "#### * Transcribe each .wav file from the audio output directory.\n",
    "#### * Store transcription segments with timestamps and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T18:59:46.698651Z",
     "iopub.status.busy": "2025-04-07T18:59:46.698322Z",
     "iopub.status.idle": "2025-04-07T19:00:09.962236Z",
     "shell.execute_reply": "2025-04-07T19:00:09.961423Z",
     "shell.execute_reply.started": "2025-04-07T18:59:46.698611Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:04<00:00, 32.7MiB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Transcribing: Experimenter_CREW_999_1_All_1731617801.wav ...\n",
      "âœ… Transcribed: Experimenter_CREW_999_1_All_1731617801.wav â†’ 78 segments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "\n",
    "def transcribe_all_audio(audio_dir):\n",
    "    \"\"\"\n",
    "    Transcribes all .wav files in a directory using Whisper.\n",
    "\n",
    "    Parameters:\n",
    "        audio_dir (str): Path to the directory containing .wav files.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from audio filename to list of transcription segments.\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"base\")  # You can try 'small' or 'medium' if GPU is available\n",
    "    transcripts = {}\n",
    "\n",
    "    for file in os.listdir(audio_dir):\n",
    "        if file.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(audio_dir, file)\n",
    "            print(f\"ðŸ” Transcribing: {file} ...\")\n",
    "            result = model.transcribe(audio_path)\n",
    "            transcripts[file] = result[\"segments\"]  # Each segment: dict with 'start', 'end', 'text'\n",
    "            print(f\"âœ… Transcribed: {file} â†’ {len(result['segments'])} segments\")\n",
    "\n",
    "    return transcripts\n",
    "\n",
    "# ðŸŸ¢ Run this on your extracted audio directory\n",
    "audio_dir = \"./output_audio\"\n",
    "all_transcriptions = transcribe_all_audio(audio_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Print the first 5 transcription segments from the first audio file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T19:01:46.448368Z",
     "iopub.status.busy": "2025-04-07T19:01:46.447889Z",
     "iopub.status.idle": "2025-04-07T19:01:46.456520Z",
     "shell.execute_reply": "2025-04-07T19:01:46.455567Z",
     "shell.execute_reply.started": "2025-04-07T19:01:46.448342Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1:\n",
      "Start: 0.0s\n",
      "End: 13.36s\n",
      "Text:  Okay, so the drive you're going to complete and use the E-cautomation and the object detection\n",
      "----------------------------------------\n",
      "Segment 2:\n",
      "Start: 13.36s\n",
      "End: 17.400000000000002s\n",
      "Text:  system so that it will not be to operate the vehicle, so keep your hands off this steering\n",
      "----------------------------------------\n",
      "Segment 3:\n",
      "Start: 17.400000000000002s\n",
      "End: 19.88s\n",
      "Text:  wheel and meet off the pedals and punch me in that drive.\n",
      "----------------------------------------\n",
      "Segment 4:\n",
      "Start: 19.88s\n",
      "End: 20.88s\n",
      "Text:  Okay.\n",
      "----------------------------------------\n",
      "Segment 5:\n",
      "Start: 20.88s\n",
      "End: 23.8s\n",
      "Text:  So when you see that some driver in the caterer highlight green, make sure you don't get\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "first_audio_file = list(all_transcriptions.keys())[0]  # Get the first file name\n",
    "first_five_segments = all_transcriptions[first_audio_file][:5]  # First 5 segments\n",
    "\n",
    "for i, segment in enumerate(first_five_segments, 1):\n",
    "    print(f\"Segment {i}:\")\n",
    "    print(f\"Start: {segment['start']}s\")\n",
    "    print(f\"End: {segment['end']}s\")\n",
    "    print(f\"Text: {segment['text']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Speaker Diarization (Using Local Pyannote Pretrained Model)** \n",
    "#### * Use a pretrained diarization pipeline from pyannote.audio (e.g., pyannote/speaker-diarization).\n",
    "#### * Run speaker diarization on each .wav file.\n",
    "#### * Assign speaker labels (e.g., Speaker 0, Speaker 1) to time-stamped segments.\n",
    "#### * Merge speaker info with Whisper transcription based on time overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T19:03:10.152785Z",
     "iopub.status.busy": "2025-04-07T19:03:10.152456Z",
     "iopub.status.idle": "2025-04-07T19:03:10.903321Z",
     "shell.execute_reply": "2025-04-07T19:03:10.902450Z",
     "shell.execute_reply.started": "2025-04-07T19:03:10.152759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"HUGGING FACE TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T19:03:27.087605Z",
     "iopub.status.busy": "2025-04-07T19:03:27.087259Z",
     "iopub.status.idle": "2025-04-07T19:27:33.108443Z",
     "shell.execute_reply": "2025-04-07T19:27:33.107569Z",
     "shell.execute_reply.started": "2025-04-07T19:03:27.087576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64ea04da9c34cedab298072eab7fab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/500 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c7d401718f47ad824de34b42fcfb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3cc3212c884b909718fe2460221f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/318 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu121. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6b93be59b64281961dd6b5ad52fe90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2bfeac7fb5490b832fb5525457c2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding_model.ckpt:   0%|          | 0.00/83.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0500905e103a4b0e877f0ccdb89d787a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mean_var_norm_emb.ckpt:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f06f3e468ad4cb8af508dfa6e128d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classifier.ckpt:   0%|          | 0.00/5.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7423866d0ab543ed9b572cc10f034940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/usr/local/lib/python3.10/dist-packages/speechbrain/processing/features.py:1529: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ™ï¸ Running diarization for Experimenter_CREW_999_1_All_1731617801.wav...\n",
      "Found only 5 clusters. Using a smaller value than 15 for `min_cluster_size` might help.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyannote/audio/pipelines/speaker_diarization.py:554: UserWarning: \n",
      "The detected number of speakers (5) is outside\n",
      "the given bounds [15, 15]. This can happen if the\n",
      "given audio file is too short to contain 15 or more speakers.\n",
      "Try to lower the desired minimal number of speakers.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Speaker assignment complete for Experimenter_CREW_999_1_All_1731617801.wav\n",
      "{'start_time': 0.0, 'end_time': 13.36, 'text': \" Okay, so the drive you're going to complete and use the E-cautomation and the object detection\", 'speaker': 'Unknown'}\n",
      "{'start_time': 13.36, 'end_time': 17.4, 'text': ' system so that it will not be to operate the vehicle, so keep your hands off this steering', 'speaker': 'SPEAKER_02'}\n",
      "{'start_time': 17.4, 'end_time': 19.88, 'text': ' wheel and meet off the pedals and punch me in that drive.', 'speaker': 'SPEAKER_02'}\n",
      "{'start_time': 19.88, 'end_time': 20.88, 'text': ' Okay.', 'speaker': 'SPEAKER_02'}\n",
      "{'start_time': 20.88, 'end_time': 23.8, 'text': \" So when you see that some driver in the caterer highlight green, make sure you don't get\", 'speaker': 'SPEAKER_02'}\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from huggingface_hub import login\n",
    "from datetime import timedelta\n",
    "\n",
    " # Replace with your actual token\n",
    "\n",
    "# Load pretrained speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=True)\n",
    "\n",
    "def diarize_audio_pyannote(audio_path):\n",
    "    \"\"\"\n",
    "    Perform speaker diarization using pyannote and return segments with speaker info.\n",
    "    \n",
    "    Parameters:\n",
    "        audio_path (str): Path to the WAV audio file.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: Speaker segments with start_time, end_time, speaker label.\n",
    "    \"\"\"\n",
    "    diarization = pipeline(audio_path, num_speakers=15)  # ðŸ’¡ Force 15 speakers\n",
    "\n",
    "    segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        segments.append({\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"speaker\": speaker\n",
    "        })\n",
    "    return segments\n",
    "def assign_speakers_to_transcripts(whisper_segments, speaker_segments):\n",
    "    \"\"\"\n",
    "    Match Whisper transcription segments to speaker segments by overlap.\n",
    "\n",
    "    Parameters:\n",
    "        whisper_segments (list): List of Whisper segments with 'start', 'end', 'text'.\n",
    "        speaker_segments (list): List of diarization segments with 'start', 'end', 'speaker'.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: Each dict contains transcription + assigned speaker.\n",
    "    \"\"\"\n",
    "    enriched = []\n",
    "    for ws in whisper_segments:\n",
    "        speaker = \"Unknown\"\n",
    "        for ss in speaker_segments:\n",
    "            if ss[\"start\"] <= ws[\"start\"] < ss[\"end\"]:\n",
    "                speaker = ss[\"speaker\"]\n",
    "                break\n",
    "        enriched.append({\n",
    "            \"start_time\": round(ws[\"start\"], 2),\n",
    "            \"end_time\": round(ws[\"end\"], 2),\n",
    "            \"text\": ws[\"text\"],\n",
    "            \"speaker\": speaker\n",
    "        })\n",
    "    return enriched\n",
    "all_transcripts_with_speakers = {}\n",
    "\n",
    "for file in os.listdir(audio_dir):\n",
    "    if file.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(audio_dir, file)\n",
    "        base_name = os.path.splitext(file)[0]\n",
    "\n",
    "        print(f\"ðŸŽ™ï¸ Running diarization for {file}...\")\n",
    "        speaker_segs = diarize_audio_pyannote(audio_path)\n",
    "\n",
    "        whisper_segs = all_transcriptions[file]  # From Step 2\n",
    "        enriched = assign_speakers_to_transcripts(whisper_segs, speaker_segs)\n",
    "\n",
    "        all_transcripts_with_speakers[base_name] = enriched\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for i, line in enumerate(enriched[:5], start=1):\n",
    "            print(f\"{i}. [{line['speaker']}] {line['text']} (Start: {line['start_time']}s, End: {line['end_time']}s)\")\n",
    "\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T19:31:55.643574Z",
     "iopub.status.busy": "2025-04-07T19:31:55.643226Z",
     "iopub.status.idle": "2025-04-07T19:31:55.650346Z",
     "shell.execute_reply": "2025-04-07T19:31:55.649458Z",
     "shell.execute_reply.started": "2025-04-07T19:31:55.643547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "1. [Unknown]  Okay, so the drive you're going to complete and use the E-cautomation and the object detection (Start: 0.0s, End: 13.36s)\n",
      "2. [SPEAKER_02]  system so that it will not be to operate the vehicle, so keep your hands off this steering (Start: 13.36s, End: 17.4s)\n",
      "3. [SPEAKER_02]  wheel and meet off the pedals and punch me in that drive. (Start: 17.4s, End: 19.88s)\n",
      "4. [SPEAKER_02]  Okay. (Start: 19.88s, End: 20.88s)\n",
      "5. [SPEAKER_02]  So when you see that some driver in the caterer highlight green, make sure you don't get (Start: 20.88s, End: 23.8s)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 60)\n",
    "for i, line in enumerate(enriched[:5], start=1):\n",
    "    print(f\"{i}. [{line['speaker']}] {line['text']} (Start: {line['start_time']}s, End: {line['end_time']}s)\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4: Bucket Transcriptions by Time**\n",
    "#### * Organize transcribed text into 5-second time buckets.\n",
    "#### * Each segment is assigned to a bucket_start and bucket_end (e.g., 0â€“5 sec, 5â€“10 sec).\n",
    "#### * Store all segments with their time buckets, speakers, and texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T19:32:41.610709Z",
     "iopub.status.busy": "2025-04-07T19:32:41.610352Z",
     "iopub.status.idle": "2025-04-07T19:32:41.618221Z",
     "shell.execute_reply": "2025-04-07T19:32:41.617604Z",
     "shell.execute_reply.started": "2025-04-07T19:32:41.610684Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning buckets for: Experimenter_CREW_999_1_All_1731617801\n",
      "Buckets assigned. First 5 entries:\n",
      "1. [Speaker: Unknown]  Okay, so the drive you're going to complete and use the E-cautomation and the object detection (Start: 0.0s, End: 13.36s) â†’ Bucket: 0-5s\n",
      "2. [Speaker: SPEAKER_02]  system so that it will not be to operate the vehicle, so keep your hands off this steering (Start: 13.36s, End: 17.4s) â†’ Bucket: 10-15s\n",
      "3. [Speaker: SPEAKER_02]  wheel and meet off the pedals and punch me in that drive. (Start: 17.4s, End: 19.88s) â†’ Bucket: 15-20s\n",
      "4. [Speaker: SPEAKER_02]  Okay. (Start: 19.88s, End: 20.88s) â†’ Bucket: 15-20s\n",
      "5. [Speaker: SPEAKER_02]  So when you see that some driver in the caterer highlight green, make sure you don't get (Start: 20.88s, End: 23.8s) â†’ Bucket: 20-25s\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def assign_buckets(transcript_segments, bucket_size=5):\n",
    "    \"\"\"\n",
    "    Adds 5-second time bucket information to each transcription segment.\n",
    "\n",
    "    Parameters:\n",
    "        transcript_segments (list): List of transcription segments with 'start_time' and 'end_time'.\n",
    "        bucket_size (int): Size of each time bucket in seconds (default: 5).\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: Updated segments with bucket_start and bucket_end.\n",
    "    \"\"\"\n",
    "    for segment in transcript_segments:\n",
    "        start_bucket = int(math.floor(segment[\"start_time\"] / bucket_size)) * bucket_size\n",
    "        end_bucket = start_bucket + bucket_size\n",
    "        segment[\"bucket_start\"] = start_bucket\n",
    "        segment[\"bucket_end\"] = end_bucket\n",
    "    return transcript_segments\n",
    "# Dictionary to store the bucketed transcriptions\n",
    "bucketed_transcripts = {}\n",
    "# Assign buckets to all transcription segments\n",
    "for file_name, segments in all_transcripts_with_speakers.items():\n",
    "    print(f\"Assigning buckets for: {file_name}\")\n",
    "    \n",
    "    # Assign buckets to the segments\n",
    "    updated_segments = assign_buckets(segments)\n",
    "    bucketed_transcripts[file_name] = updated_segments\n",
    "    \n",
    "    # Print the first 5 segments \n",
    "    print(f\"Buckets assigned. First 5 entries:\")\n",
    "    for i, segment in enumerate(updated_segments[:5], start=1):\n",
    "        print(f\"{i}. [Speaker: {segment['speaker']}] {segment['text']} (Start: {segment['start_time']}s, End: {segment['end_time']}s) â†’ Bucket: {segment['bucket_start']}-{segment['bucket_end']}s\")\n",
    "\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 5: Sentiment Analysis (Using NLTK VADER)**\n",
    "#### * For each transcribed segment, analyze sentiment using the VADER sentiment analyzer.\n",
    "#### * Assign a label: positive, negative, or neutral based on compound score.\n",
    "#### * Add sentiment to the segment metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T19:32:46.205879Z",
     "iopub.status.busy": "2025-04-07T19:32:46.205586Z",
     "iopub.status.idle": "2025-04-07T19:32:46.226954Z",
     "shell.execute_reply": "2025-04-07T19:32:46.226207Z",
     "shell.execute_reply.started": "2025-04-07T19:32:46.205854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      " Analyzing sentiment for: Experimenter_CREW_999_1_All_1731617801\n",
      "Sentiment analysis complete. First 5 segments:\n",
      "1. [Speaker: Unknown]  Okay, so the drive you're going to complete and use the E-cautomation and the object detection (Sentiment: positive)\n",
      "   Start Time: 0.0s, End Time: 13.36s\n",
      "   Bucket: 0 - 5s\n",
      "2. [Speaker: SPEAKER_02]  system so that it will not be to operate the vehicle, so keep your hands off this steering (Sentiment: neutral)\n",
      "   Start Time: 13.36s, End Time: 17.4s\n",
      "   Bucket: 10 - 15s\n",
      "3. [Speaker: SPEAKER_02]  wheel and meet off the pedals and punch me in that drive. (Sentiment: neutral)\n",
      "   Start Time: 17.4s, End Time: 19.88s\n",
      "   Bucket: 15 - 20s\n",
      "4. [Speaker: SPEAKER_02]  Okay. (Sentiment: positive)\n",
      "   Start Time: 19.88s, End Time: 20.88s\n",
      "   Bucket: 15 - 20s\n",
      "5. [Speaker: SPEAKER_02]  So when you see that some driver in the caterer highlight green, make sure you don't get (Sentiment: positive)\n",
      "   Start Time: 20.88s, End Time: 23.8s\n",
      "   Bucket: 20 - 25s\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK VADER lexicon (only needed once)\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyzes sentiment of a given text using VADER SentimentIntensityAnalyzer.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        str: Sentiment label: \"positive\", \"negative\", or \"neutral\".\n",
    "    \"\"\"\n",
    "    # Analyze sentiment\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "    # Assign sentiment based on the compound score\n",
    "    if sentiment_score >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def apply_sentiment_to_segments(transcript_segments):\n",
    "    \"\"\"\n",
    "    Applies sentiment analysis to each transcription segment.\n",
    "\n",
    "    Parameters:\n",
    "        transcript_segments (list): List of transcription segments with 'text' and other info.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated transcription segments with sentiment.\n",
    "    \"\"\"\n",
    "    for segment in transcript_segments:\n",
    "        segment[\"sentiment\"] = analyze_sentiment(segment[\"text\"])\n",
    "    return transcript_segments\n",
    "\n",
    "# Dictionary to store sentiment-analyzed transcripts\n",
    "sentimented_transcripts = {}\n",
    "\n",
    "# Apply sentiment analysis for all files\n",
    "for file_name, segments in bucketed_transcripts.items():\n",
    "    print(f\" Analyzing sentiment for: {file_name}\")\n",
    "    \n",
    "    # Apply sentiment analysis to segments\n",
    "    updated_segments_with_sentiment = apply_sentiment_to_segments(segments)\n",
    "    sentimented_transcripts[file_name] = updated_segments_with_sentiment\n",
    "    \n",
    "    # Print the first 5 segments\n",
    "    print(f\"Sentiment analysis complete. First 5 segments:\")\n",
    "    \n",
    "    for i, entry in enumerate(updated_segments_with_sentiment[:5], start=1):\n",
    "        print(f\"{i}. [Speaker: {entry['speaker']}] {entry['text']} (Sentiment: {entry['sentiment']})\")\n",
    "        print(f\"   Start Time: {entry['start_time']}s, End Time: {entry['end_time']}s\")\n",
    "        print(f\"   Bucket: {entry['bucket_start']} - {entry['bucket_end']}s\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 6: Named Entity Recognition (NER)**\n",
    "#### * Use a pre-trained Hugging Face model (e.g., dslim/bert-base-NER).\n",
    "#### * Detect named entities (people, organizations, places) in each transcription segment.\n",
    "#### * Store named entities alongside their corresponding segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T19:32:58.890343Z",
     "iopub.status.busy": "2025-04-07T19:32:58.890056Z",
     "iopub.status.idle": "2025-04-07T19:33:27.765887Z",
     "shell.execute_reply": "2025-04-07T19:33:27.764990Z",
     "shell.execute_reply.started": "2025-04-07T19:32:58.890319Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac3a4fe24254bbb9310ded3cf644b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23182691a7046e29208fa504d24c04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d5fa7cd34f4ac6931c72e60cde3f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5cdf149a9b43c9a13972f43ffc1689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Extracting named entities for: Experimenter_CREW_999_1_All_1731617801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NER complete for Experimenter_CREW_999_1_All_1731617801. First 3 segments with named entities:\n",
      "{'start_time': 0.0, 'end_time': 13.36, 'text': \" Okay, so the drive you're going to complete and use the E-cautomation and the object detection\", 'speaker': 'Unknown', 'bucket_start': 0, 'bucket_end': 5, 'sentiment': 'positive', 'named_entities': []}\n",
      "{'start_time': 13.36, 'end_time': 17.4, 'text': ' system so that it will not be to operate the vehicle, so keep your hands off this steering', 'speaker': 'SPEAKER_02', 'bucket_start': 10, 'bucket_end': 15, 'sentiment': 'neutral', 'named_entities': []}\n",
      "{'start_time': 17.4, 'end_time': 19.88, 'text': ' wheel and meet off the pedals and punch me in that drive.', 'speaker': 'SPEAKER_02', 'bucket_start': 15, 'bucket_end': 20, 'sentiment': 'neutral', 'named_entities': []}\n",
      "\n",
      "ðŸ“„ First 5 segments with Named Entities from Experimenter_CREW_999_1_All_1731617801:\n",
      "1. [Speaker: Unknown]  Okay, so the drive you're going to complete and use the E-cautomation and the object detection (Sentiment: positive)\n",
      "   Named Entities: \n",
      "   Start Time: 0.0s, End Time: 13.36s\n",
      "   Bucket: 0 - 5s\n",
      "2. [Speaker: SPEAKER_02]  system so that it will not be to operate the vehicle, so keep your hands off this steering (Sentiment: neutral)\n",
      "   Named Entities: \n",
      "   Start Time: 13.36s, End Time: 17.4s\n",
      "   Bucket: 10 - 15s\n",
      "3. [Speaker: SPEAKER_02]  wheel and meet off the pedals and punch me in that drive. (Sentiment: neutral)\n",
      "   Named Entities: \n",
      "   Start Time: 17.4s, End Time: 19.88s\n",
      "   Bucket: 15 - 20s\n",
      "4. [Speaker: SPEAKER_02]  Okay. (Sentiment: positive)\n",
      "   Named Entities: \n",
      "   Start Time: 19.88s, End Time: 20.88s\n",
      "   Bucket: 15 - 20s\n",
      "5. [Speaker: SPEAKER_02]  So when you see that some driver in the caterer highlight green, make sure you don't get (Sentiment: positive)\n",
      "   Named Entities: \n",
      "   Start Time: 20.88s, End Time: 23.8s\n",
      "   Bucket: 20 - 25s\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the Hugging Face NER pipeline\n",
    "ner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Function to extract named entities from text\n",
    "def extract_named_entities(text):\n",
    "    \"\"\"\n",
    "    Extracts named entities from the given text using a pre-trained NER model.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of named entities found in the text.\n",
    "    \"\"\"\n",
    "    ner_results = ner_model(text)\n",
    "    # Extract words (entities) from the NER output\n",
    "    named_entities = [result['word'] for result in ner_results]\n",
    "    return named_entities\n",
    "\n",
    "# Function to apply NER to each transcription segment\n",
    "def apply_ner_to_segments(transcript_segments):\n",
    "    \"\"\"\n",
    "    Applies Named Entity Recognition (NER) to each transcription segment.\n",
    "\n",
    "    Parameters:\n",
    "        transcript_segments (list): List of transcription segments.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated transcription segments with named entities.\n",
    "    \"\"\"\n",
    "    for segment in transcript_segments:\n",
    "        # Apply NER to each segment's text\n",
    "        segment[\"named_entities\"] = extract_named_entities(segment[\"text\"])\n",
    "    return transcript_segments\n",
    "\n",
    "# Example usage\n",
    "# Assuming `sentimented_transcripts` is the dictionary containing all transcribed segments along with sentiment\n",
    "nered_transcripts = {}\n",
    "\n",
    "for file_name, segments in sentimented_transcripts.items():\n",
    "    print(f\"ðŸ” Extracting named entities for: {file_name}\")\n",
    "    updated_segments_with_ner = apply_ner_to_segments(segments)\n",
    "    nered_transcripts[file_name] = updated_segments_with_ner\n",
    "    print(f\"âœ… NER complete for {file_name}. First 3 segments with named entities:\")\n",
    "    \n",
    "    # Display the first 3 segments with named entities\n",
    "    for entry in updated_segments_with_ner[:3]:\n",
    "        print(entry)\n",
    "for file_name, segments in nered_transcripts.items():\n",
    "    print(f\"\\nðŸ“„ First 5 segments with Named Entities from {file_name}:\")\n",
    "    for i, entry in enumerate(segments[:5], start=1):\n",
    "        print(f\"{i}. [Speaker: {entry['speaker']}] {entry['text']} (Sentiment: {entry['sentiment']})\")\n",
    "        print(f\"   Named Entities: {', '.join(entry['named_entities'])}\")\n",
    "        print(f\"   Start Time: {entry['start_time']}s, End Time: {entry['end_time']}s\")\n",
    "        print(f\"   Bucket: {entry['bucket_start']} - {entry['bucket_end']}s\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 7: Export Results to CSV**\n",
    "#### * Combine all enriched segment data into rows (start time, end time, speaker, text, sentiment, named entities, etc.).\n",
    "#### * Export the final dataset into a directory where individual CSVs will be saved.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def export_per_video_csvs(nered_transcripts, output_dir=\"./csv_outputs\"):\n",
    "    \"\"\"\n",
    "    Exports a separate CSV file for each video's enriched transcript.\n",
    "\n",
    "    Parameters:\n",
    "        nered_transcripts (dict): Dict containing data per video with NER & sentiment.\n",
    "        output_dir (str): Directory where individual CSVs will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file_name, segments in nered_transcripts.items():\n",
    "        rows = []\n",
    "        for segment in segments:\n",
    "            row = {\n",
    "                \"start_time\": segment[\"start_time\"],\n",
    "                \"end_time\": segment[\"end_time\"],\n",
    "                \"bucket_start\": int(segment[\"start_time\"] // 5) * 5,\n",
    "                \"bucket_end\": (int(segment[\"start_time\"] // 5) + 1) * 5,\n",
    "                \"text\": segment[\"text\"],\n",
    "                \"sentiment\": segment[\"sentiment\"],\n",
    "                \"named_entities\": \", \".join(segment[\"named_entities\"]),\n",
    "                \"word_count\": len(segment[\"text\"].split()),\n",
    "                \"speaker\": segment[\"speaker\"]\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "        output_path = os.path.join(output_dir, f\"{file_name}.csv\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"âœ… Exported: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7011662,
     "sourceId": 11226250,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7072949,
     "sourceId": 11309214,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
