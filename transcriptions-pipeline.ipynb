{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13883785,"sourceType":"datasetVersion","datasetId":8845723}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-build-isolation --index-url https://download.pytorch.org/whl/cu121 \\\n    torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0\n\n!pip install \\\n    pyannote.audio==3.3.1 \\\n    openai-whisper==20231117 \\\n    transformers==4.34.0 \\\n    huggingface-hub==0.16.4 \\\n    tokenizers==0.14.1 \\\n    ffmpeg-python==0.2.0 \\\n    librosa==0.10.1 \\\n    pandas==2.2.2 \\\n    nltk==3.8.1 \\\n    seaborn==0.12.2 \\\n    matplotlib==3.7.1\n\n!pip install speechbrain==0.5.15 \\\n    resemblyzer==0.1.4 \\\n    webrtcvad==2.0.10 \\\n    scikit-learn==1.2.2 \\\n    soundfile==0.13.1\n\n# spaCy and model (NER fallback, no deps upgrades!)\n!pip install -q spacy==3.6.1 --no-deps\n!pip install -q https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl --no-deps\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:18:44.036734Z","iopub.execute_input":"2025-11-26T17:18:44.036947Z","iopub.status.idle":"2025-11-26T17:23:05.044578Z","shell.execute_reply.started":"2025-11-26T17:18:44.036919Z","shell.execute_reply":"2025-11-26T17:23:05.043623Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu121\nCollecting torch==2.2.0\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp311-cp311-linux_x86_64.whl (757.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.17.0\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==2.2.0\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (4.15.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.0)\n  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (2.32.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17.0) (11.3.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.5.82)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0) (3.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17.0) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17.0) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17.0) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17.0) (2025.10.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0) (1.3.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.17.0) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.17.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.17.0) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.17.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.17.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.17.0) (2024.2.0)\nInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchaudio, torchvision\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.0+cu121 torchaudio-2.2.0+cu121 torchvision-0.17.0+cu121 triton-2.2.0\nCollecting pyannote.audio==3.3.1\n  Downloading pyannote.audio-3.3.1-py2.py3-none-any.whl.metadata (11 kB)\nCollecting openai-whisper==20231117\n  Downloading openai-whisper-20231117.tar.gz (798 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting transformers==4.34.0\n  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting huggingface-hub==0.16.4\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\nCollecting tokenizers==0.14.1\n  Downloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting ffmpeg-python==0.2.0\n  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nCollecting librosa==0.10.1\n  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\nCollecting pandas==2.2.2\n  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\nCollecting nltk==3.8.1\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: seaborn==0.12.2 in /usr/local/lib/python3.11/dist-packages (0.12.2)\nCollecting matplotlib==3.7.1\n  Downloading matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nCollecting asteroid-filterbanks>=0.4 (from pyannote.audio==3.3.1)\n  Downloading asteroid_filterbanks-0.4.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==3.3.1) (0.8.1)\nCollecting lightning>=2.0.1 (from pyannote.audio==3.3.1)\n  Downloading lightning-2.5.6-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==3.3.1) (2.3.0)\nCollecting pyannote.core>=5.0.0 (from pyannote.audio==3.3.1)\n  Downloading pyannote_core-6.0.1-py3-none-any.whl.metadata (1.9 kB)\nCollecting pyannote.database>=5.0.1 (from pyannote.audio==3.3.1)\n  Downloading pyannote_database-6.1.0-py3-none-any.whl.metadata (30 kB)\nCollecting pyannote.metrics>=3.2 (from pyannote.audio==3.3.1)\n  Downloading pyannote_metrics-4.0.0-py3-none-any.whl.metadata (2.2 kB)\nCollecting pyannote.pipeline>=3.0.1 (from pyannote.audio==3.3.1)\n  Downloading pyannote_pipeline-4.0.0-py3-none-any.whl.metadata (5.4 kB)\nCollecting pytorch-metric-learning>=2.1.0 (from pyannote.audio==3.3.1)\n  Downloading pytorch_metric_learning-2.9.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==3.3.1) (14.2.0)\nRequirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==3.3.1) (3.0.4)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==3.3.1) (0.13.1)\nCollecting speechbrain>=1.0.0 (from pyannote.audio==3.3.1)\n  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\nCollecting tensorboardX>=2.6 (from pyannote.audio==3.3.1)\n  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==3.3.1) (2.2.0+cu121)\nCollecting torch-audiomentations>=0.11.0 (from pyannote.audio==3.3.1)\n  Downloading torch_audiomentations-0.12.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torchaudio>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==3.3.1) (2.2.0+cu121)\nRequirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.audio==3.3.1) (1.8.2)\nRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20231117) (2.2.0)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20231117) (0.60.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20231117) (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20231117) (4.67.1)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20231117) (10.7.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20231117) (0.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (3.20.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (2.32.5)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0) (0.5.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.16.4) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.16.4) (4.15.0)\nRequirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python==0.2.0) (1.0.0)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.1) (3.0.1)\nRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.1) (1.15.3)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.1) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.1) (1.5.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.1) (4.4.2)\nRequirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.1) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.1) (0.5.0.post1)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.1) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.10.1) (1.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (11.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (3.0.9)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0.1->pyannote.audio==3.3.1) (0.15.2)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0.1->pyannote.audio==3.3.1) (2.5.5)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper==20231117) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper==20231117) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper==20231117) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper==20231117) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper==20231117) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper==20231117) (2.4.1)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio==3.3.1) (4.9.3)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.0->librosa==0.10.1) (4.5.0)\nCollecting numpy (from openai-whisper==20231117)\n  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hINFO: pip is looking at multiple versions of pyannote-core to determine which version is compatible with other requirements. This could take a while.\nCollecting pyannote.core>=5.0.0 (from pyannote.audio==3.3.1)\n  Downloading pyannote_core-6.0.0-py3-none-any.whl.metadata (1.9 kB)\n  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.3.1) (2.4.0)\nINFO: pip is looking at multiple versions of pyannote-database to determine which version is compatible with other requirements. This could take a while.\nCollecting pyannote.database>=5.0.1 (from pyannote.audio==3.3.1)\n  Downloading pyannote_database-6.0.0-py3-none-any.whl.metadata (30 kB)\n  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote.audio==3.3.1) (0.16.0)\nINFO: pip is looking at multiple versions of pyannote-metrics to determine which version is compatible with other requirements. This could take a while.\nCollecting pyannote.metrics>=3.2 (from pyannote.audio==3.3.1)\n  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\nCollecting docopt>=0.6.2 (from pyannote.metrics>=3.2->pyannote.audio==3.3.1)\n  Downloading docopt-0.6.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.3.1) (0.9.0)\nRequirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.3.1) (1.13.1)\nRequirement already satisfied: optuna>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (4.5.0)\nCollecting pyannote.pipeline>=3.0.1 (from pyannote.audio==3.3.1)\n  Downloading pyannote.pipeline-3.0.1-py3-none-any.whl.metadata (897 bytes)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0) (2025.10.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio==3.3.1) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote.audio==3.3.1) (2.19.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa==0.10.1) (3.6.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote.audio==3.3.1) (2.0.0)\nCollecting hyperpyyaml (from speechbrain>=1.0.0->pyannote.audio==3.3.1)\n  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote.audio==3.3.1) (0.2.0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.6->pyannote.audio==3.3.1) (6.33.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->pyannote.audio==3.3.1) (12.1.105)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pyannote.audio==3.3.1) (12.5.82)\nCollecting julius<0.3,>=0.2.3 (from torch-audiomentations>=0.11.0->pyannote.audio==3.3.1)\n  Downloading julius-0.2.7.tar.gz (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting torch-pitch-shift>=1.2.2 (from torch-audiomentations>=0.11.0->pyannote.audio==3.3.1)\n  Downloading torch_pitch_shift-1.2.5-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio==3.3.1) (2.23)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.3.1) (3.13.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio==3.3.1) (75.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio==3.3.1) (0.1.2)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.0->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (1.17.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.0->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (6.10.1)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.0->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (2.0.41)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio==3.3.1) (1.3.0)\nCollecting primePy>=1.3 (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio==3.3.1)\n  Downloading primePy-1.3-py3-none-any.whl.metadata (4.8 kB)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio==3.3.1) (1.5.4)\nRequirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio==3.3.1) (0.18.16)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio==3.3.1) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper==20231117) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper==20231117) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper==20231117) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper==20231117) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->openai-whisper==20231117) (2024.2.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.3.1) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.3.1) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.3.1) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.3.1) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.3.1) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.3.1) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.3.1) (1.22.0)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna>=4.2.0->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (1.3.10)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->openai-whisper==20231117) (2024.2.0)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio==3.3.1) (0.2.14)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=4.2.0->pyannote.pipeline>=3.0.1->pyannote.audio==3.3.1) (3.2.3)\nDownloading pyannote.audio-3.3.1-py2.py3-none-any.whl (898 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.7/898.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\nDownloading librosa-0.10.1-py3-none-any.whl (253 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading asteroid_filterbanks-0.4.0-py3-none-any.whl (29 kB)\nDownloading lightning-2.5.6-py3-none-any.whl (827 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.9/827.9 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyannote.pipeline-3.0.1-py3-none-any.whl (31 kB)\nDownloading pytorch_metric_learning-2.9.0-py3-none-any.whl (127 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch_audiomentations-0.12.0-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch_pitch_shift-1.2.5-py3-none-any.whl (5.0 kB)\nDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\nDownloading primePy-1.3-py3-none-any.whl (4.0 kB)\nBuilding wheels for collected packages: openai-whisper, docopt, julius\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801443 sha256=c61264ec5207c7fcec732520fab6fe52b5169823caea6dd8f95d5a790d4f5baa\n  Stored in directory: /root/.cache/pip/wheels/55/5d/42/c296ab046d52caa0adc0e3f159e98f011b3994a022d6282105\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e1617a214fa732895ae34eccafc7c9b46926845f0bf9a81e42c8de9435480fae\n  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n  Building wheel for julius (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for julius: filename=julius-0.2.7-py3-none-any.whl size=21870 sha256=4384e1e9f85c455e5ce362a90059a1e493647892c57ea610baafbd2b3531f667\n  Stored in directory: /root/.cache/pip/wheels/16/15/d4/edd724cefe78050a6ba3344b8b0c6672db829a799dbb9f81ff\nSuccessfully built openai-whisper docopt julius\nInstalling collected packages: primePy, docopt, nltk, ffmpeg-python, hyperpyyaml, huggingface-hub, tokenizers, julius, torch-pitch-shift, torch-audiomentations, pyannote.core, pandas, pyannote.database, matplotlib, tensorboardX, speechbrain, pytorch-metric-learning, pyannote.pipeline, pyannote.metrics, lightning, asteroid-filterbanks, transformers, pyannote.audio, openai-whisper, librosa\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.9.2\n    Uninstalling nltk-3.9.2:\n      Successfully uninstalled nltk-3.9.2\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.36.0\n    Uninstalling huggingface-hub-0.36.0:\n      Successfully uninstalled huggingface-hub-0.36.0\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.3\n    Uninstalling pandas-2.2.3:\n      Successfully uninstalled pandas-2.2.3\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.2\n    Uninstalling matplotlib-3.7.2:\n      Successfully uninstalled matplotlib-3.7.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: librosa\n    Found existing installation: librosa 0.11.0\n    Uninstalling librosa-0.11.0:\n      Successfully uninstalled librosa-0.11.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.1 requires huggingface-hub<2.0,>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ngradio-client 1.11.0 requires huggingface-hub>=0.19.3, but you have huggingface-hub 0.16.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nsentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.34.0 which is incompatible.\ndiffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.16.4 which is incompatible.\npeft 0.16.0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\ngradio 5.38.1 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.16.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ntextblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\naccelerate 1.9.0 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.16.4 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asteroid-filterbanks-0.4.0 docopt-0.6.2 ffmpeg-python-0.2.0 huggingface-hub-0.16.4 hyperpyyaml-1.2.2 julius-0.2.7 librosa-0.10.1 lightning-2.5.6 matplotlib-3.7.1 nltk-3.8.1 openai-whisper-20231117 pandas-2.2.2 primePy-1.3 pyannote.audio-3.3.1 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyannote.pipeline-3.0.1 pytorch-metric-learning-2.9.0 speechbrain-1.0.3 tensorboardX-2.6.4 tokenizers-0.14.1 torch-audiomentations-0.12.0 torch-pitch-shift-1.2.5 transformers-4.34.0\nCollecting speechbrain==0.5.15\n  Downloading speechbrain-0.5.15-py3-none-any.whl.metadata (23 kB)\nCollecting resemblyzer==0.1.4\n  Downloading Resemblyzer-0.1.4-py3-none-any.whl.metadata (5.8 kB)\nCollecting webrtcvad==2.0.10\n  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: soundfile==0.13.1 in /usr/local/lib/python3.11/dist-packages (0.13.1)\nRequirement already satisfied: hyperpyyaml in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (1.2.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (1.5.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (25.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (1.15.3)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (0.2.0)\nRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (2.2.0+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (2.2.0+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from speechbrain==0.5.15) (0.16.4)\nRequirement already satisfied: librosa>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from resemblyzer==0.1.4) (0.10.1)\nCollecting typing (from resemblyzer==0.1.4)\n  Downloading typing-3.7.4.3.tar.gz (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile==0.13.1) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile==0.13.1) (2.23)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer==0.1.4) (3.0.1)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer==0.1.4) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer==0.1.4) (0.60.0)\nRequirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer==0.1.4) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer==0.1.4) (0.5.0.post1)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer==0.1.4) (4.15.0)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer==0.1.4) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->resemblyzer==0.1.4) (1.1.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain==0.5.15) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain==0.5.15) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain==0.5.15) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain==0.5.15) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain==0.5.15) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain==0.5.15) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (3.20.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain==0.5.15) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->speechbrain==0.5.15) (12.5.82)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->speechbrain==0.5.15) (2.32.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->speechbrain==0.5.15) (6.0.3)\nRequirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain==0.5.15) (0.18.16)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa>=0.9.1->resemblyzer==0.1.4) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.0->librosa>=0.9.1->resemblyzer==0.1.4) (4.5.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->speechbrain==0.5.15) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->speechbrain==0.5.15) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->speechbrain==0.5.15) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->speechbrain==0.5.15) (2025.10.5)\nRequirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain==0.5.15) (0.2.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->speechbrain==0.5.15) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->speechbrain==0.5.15) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->speechbrain==0.5.15) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->speechbrain==0.5.15) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->speechbrain==0.5.15) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->speechbrain==0.5.15) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.9->speechbrain==0.5.15) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->speechbrain==0.5.15) (2024.2.0)\nDownloading speechbrain-0.5.15-py3-none-any.whl (553 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.8/553.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading Resemblyzer-0.1.4-py3-none-any.whl (15.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: webrtcvad, typing\n  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp311-cp311-linux_x86_64.whl size=73508 sha256=d5d12e463a4170f74e6127b339bf0fdf31767436c8e715d46567b98940eafde7\n  Stored in directory: /root/.cache/pip/wheels/94/65/3f/292d0b656be33d1c801831201c74b5f68f41a2ae465ff2ee2f\n  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=a5b18ec0f6bba8b6eb9310e6e1e3c983a000722dc70be5a97c8e2a6012c5ea83\n  Stored in directory: /root/.cache/pip/wheels/9d/67/2f/53e3ef32ec48d11d7d60245255e2d71e908201d20c880c08ee\nSuccessfully built webrtcvad typing\nInstalling collected packages: webrtcvad, typing, speechbrain, resemblyzer\n  Attempting uninstall: speechbrain\n    Found existing installation: speechbrain 1.0.3\n    Uninstalling speechbrain-1.0.3:\n      Successfully uninstalled speechbrain-1.0.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npyannote-audio 3.3.1 requires speechbrain>=1.0.0, but you have speechbrain 0.5.15 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed resemblyzer-0.1.4 speechbrain-0.5.15 typing-3.7.4.3 webrtcvad-2.0.10\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport subprocess\nimport math\nimport json\nimport re\nimport ffmpeg\nimport librosa\nimport soundfile as sf\nimport whisper\nimport numpy as np\nfrom resemblyzer import VoiceEncoder\nfrom sklearn.cluster import AgglomerativeClustering\nimport pandas as pd\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:24:03.038654Z","iopub.execute_input":"2025-11-26T17:24:03.038961Z","iopub.status.idle":"2025-11-26T17:24:09.990403Z","shell.execute_reply.started":"2025-11-26T17:24:03.038928Z","shell.execute_reply":"2025-11-26T17:24:09.989731Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# **Step-by-Step Workflow**","metadata":{}},{"cell_type":"markdown","source":"## **Step 1: Data Ingestion**\n#### Loads all video files from a directory, extracts audio using FFmpeg, and saves each audio file as mono 16kHz WAV format for consistent downstream processing.\n\n## **Step 2: Transcription**\n#### Uses OpenAI Whisper (offline, local model) to transcribe each audio file into text, returning segments with start and end timestamps.\n\n## **Step 3: Speaker Diarization**\n#### Uses Resemblyzer to extract speaker embeddings from each segment and applies Agglomerative Clustering to assign speaker labels, distinguishing between different voices in the audio.\n\n## **Step 4: Time Bucketing**\n#### Divides each transcribed segment into fixed 5-second time buckets using the start timestamp, enabling temporal aggregation and analysis.\n\n## **Step 5: Sentiment Analysis**\n#### Applies NLTK's VADER sentiment analyzer to classify each text segment as \"positive\", \"negative\", or \"neutral\".\n\n## **Step 6: Named Entity Recognition (NER)**\n#### Applies a custom regex-based Named Entity Recognition (NER) method to extract emails, URLs, dates, money, phone numbers, names, organizations, and locations — without using any external ML model.\n\n## **Step 7: Export to CSV**\n#### Aggregates all enriched segment information (timestamps, text, speakers, sentiment, entities, word count, and time buckets) into separate CSV files — one per audio — for downstream analysis or visualization.\n","metadata":{}},{"cell_type":"markdown","source":"# **Step 1: Data Ingestion (Video to Audio Conversion)** \n#### * Load video files from the input directory.\n#### * Extract audio from each video using ffmpeg.\n#### * Convert audio to mono and 16kHz .wav format.\n#### * Store the resulting audio files in an output folder (e.g., ./output_audio).","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\n\ndef extract_audio(video_path, output_audio_path):\n    \"\"\"\n    Extracts audio from a single video and saves it as mono 16kHz WAV.\n\n    Parameters:\n        video_path (str): Path to the input video file.\n        output_audio_path (str): Path where the output audio will be saved.\n\n    Returns:\n        str: Path to the saved audio file.\n    \"\"\"\n    cmd = [\n        \"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\",\n        \"-ac\", \"1\", \"-ar\", \"16000\",  # Mono & 16kHz\n        output_audio_path, \"-y\"\n    ]\n    subprocess.run(cmd, capture_output=True, text=True)\n    return output_audio_path\n\ndef process_all_videos(input_video_dir):\n    \"\"\"\n    Processes all video files in the input directory,\n    extracts audio, and saves them to ./output_audio/.\n\n    Parameters:\n        input_video_dir (str): Directory containing video files.\n    \"\"\"\n    output_audio_dir = \"./output_audio\"\n    os.makedirs(output_audio_dir, exist_ok=True)\n\n    video_extensions = (\".mp4\", \".mov\", \".avi\", \".mkv\")\n\n    for filename in os.listdir(input_video_dir):\n        if filename.lower().endswith(video_extensions):\n            video_path = os.path.join(input_video_dir, filename)\n            base_name = os.path.splitext(filename)[0]\n            output_audio_path = os.path.join(output_audio_dir, f\"{base_name}.wav\")\n\n            extract_audio(video_path, output_audio_path)\n            print(f\"✅ Processed: {filename} → {output_audio_path}\")\n\n# 🟡 Call this function with your actual input folder containing videos\ninput_video_directory = \"/kaggle/input/videos\"\nprocess_all_videos(input_video_directory)  # Change path as needed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:27:25.350553Z","iopub.execute_input":"2025-11-26T17:27:25.351371Z","iopub.status.idle":"2025-11-26T17:27:27.142491Z","shell.execute_reply.started":"2025-11-26T17:27:25.351342Z","shell.execute_reply":"2025-11-26T17:27:27.141858Z"}},"outputs":[{"name":"stdout","text":"✅ Processed: video_convo.mp4 → ./output_audio/video_convo.wav\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Step 2: Transcription (Using Whisper)**\n#### * Load a pre-trained Whisper model (base, small, etc.).\n#### * Transcribe each .wav file from the audio output directory.\n#### * Store transcription segments with timestamps and text.","metadata":{}},{"cell_type":"code","source":"def transcribe_all_audio(audio_dir):\n    \"\"\"\n    Transcribes all .wav files in a directory using Whisper.\n\n    Parameters:\n        audio_dir (str): Path to the directory containing .wav files.\n\n    Returns:\n        dict: Mapping from audio filename to list of transcription segments.\n    \"\"\"\n    model = whisper.load_model(\"base\")  # You can try 'small' or 'medium' if GPU is available\n    transcripts = {}\n\n    for file in os.listdir(audio_dir):\n        if file.endswith(\".wav\"):\n            audio_path = os.path.join(audio_dir, file)\n            print(f\"🔍 Transcribing: {file} ...\")\n            result = model.transcribe(audio_path)\n            transcripts[file] = result[\"segments\"]  # Each segment: dict with 'start', 'end', 'text'\n            print(f\"✅ Transcribed: {file} → {len(result['segments'])} segments\")\n\n    return transcripts\n\n# 🟢 Run this on your extracted audio directory\naudio_dir = \"./output_audio\"\nall_transcriptions = transcribe_all_audio(audio_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:28:10.485468Z","iopub.execute_input":"2025-11-26T17:28:10.486252Z","iopub.status.idle":"2025-11-26T17:28:16.995992Z","shell.execute_reply.started":"2025-11-26T17:28:10.486224Z","shell.execute_reply":"2025-11-26T17:28:16.995317Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 133MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"🔍 Transcribing: video_convo.wav ...\n✅ Transcribed: video_convo.wav → 13 segments\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"first_audio_file = list(all_transcriptions.keys())[0]  # Get the first file name\nfirst_five_segments = all_transcriptions[first_audio_file][:5]  # First 5 segments\n\nfor i, segment in enumerate(first_five_segments, 1):\n    print(f\"Segment {i}:\")\n    print(f\"Start: {segment['start']}s\")\n    print(f\"End: {segment['end']}s\")\n    print(f\"Text: {segment['text']}\")\n    print(\"-\" * 40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:28:41.939235Z","iopub.execute_input":"2025-11-26T17:28:41.939523Z","iopub.status.idle":"2025-11-26T17:28:41.945037Z","shell.execute_reply.started":"2025-11-26T17:28:41.939504Z","shell.execute_reply":"2025-11-26T17:28:41.944333Z"}},"outputs":[{"name":"stdout","text":"Segment 1:\nStart: 0.0s\nEnd: 5.28s\nText:  It's a beautiful house. Do you both live here? Yes. Can I get a short house tour? Yeah, mind your shirt.\n----------------------------------------\nSegment 2:\nStart: 7.12s\nEnd: 12.32s\nText:  Welcome to our home. Sat Tichit means true consciousness. This is the house which we will\n----------------------------------------\nSegment 3:\nStart: 12.32s\nEnd: 18.0s\nText:  keep in mind that it enhances our consciousness. These are the waste bottles used to build this mud wall,\n----------------------------------------\nSegment 4:\nStart: 18.0s\nEnd: 21.92s\nText:  the colourful reflection on the backside. Oh wow, you have a hand pump at the end. Yeah.\n----------------------------------------\nSegment 5:\nStart: 23.04s\nEnd: 27.36s\nText:  We use the rain water to write this. Let's go.\n----------------------------------------\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **Step 3: Speaker Diarization (Using Resemblyzer + Clustering)**  \n#### * Load each audio file and extract its waveform at 16kHz mono.  \n#### * For every Whisper transcription segment, compute a speaker embedding using Resemblyzer’s pretrained VoiceEncoder.  \n#### * Group similar embeddings using Agglomerative Clustering to identify distinct speakers (e.g., spk_0, spk_1, spk_2).  \n#### * Assign a speaker label to each transcription segment based on which cluster its embedding belongs to.  \n#### * Optionally merge consecutive segments spoken by the same speaker to form cleaner dialogue turns.\n","metadata":{}},{"cell_type":"code","source":"# Diarization via embeddings + clustering (resemblyzer)\n\naudio_dir = \"./output_audio\"   # same as before\nencoder = VoiceEncoder()       # loads pre-trained encoder (small & fast)\n\ndef load_wave_16k_mono(path):\n    # returns numpy float32 waveform sampled at 16k mono (range -1..1)\n    wav, sr = librosa.load(path, sr=16000, mono=True)\n    return wav.astype(np.float32), 16000\n\ndef embed_segment(waveform, sr, start_s, end_s):\n    # waveform is 1D numpy at sr (assumed 16k)\n    s = int(start_s * sr)\n    e = int(end_s * sr)\n    chunk = waveform[s:e]\n    if len(chunk) < 160:  # tiny guard\n        # pad tiny segments\n        chunk = np.pad(chunk, (0, max(0, 160-len(chunk))), mode='constant')\n    # resemblyzer expects float32 in range [-1,1]\n    try:\n        emb = encoder.embed_utterance(chunk)\n    except Exception:\n        # fallback: if chunk too small or encoder throws, use zero vector\n        emb = np.zeros((encoder.embedding_size,))\n    return emb\n\ndef diarize_file_by_whisper(wav_path, whisper_segments, estimate_speakers=None):\n    \"\"\"\n    whisper_segments: list of dicts {'start':float,'end':float,'text':str}\n    returns: list of dicts {'start':..,'end':..,'text':..,'speaker':..}\n    \"\"\"\n    waveform, sr = load_wave_16k_mono(wav_path)\n\n    # compute embeddings for each whisper segment\n    embeddings = []\n    valid_segments = []\n    for seg in whisper_segments:\n        # Skip empty text segments if you prefer:\n        # if not seg.get(\"text\",\"\").strip(): continue\n        emb = embed_segment(waveform, sr, seg['start'], seg['end'])\n        embeddings.append(emb)\n        valid_segments.append(seg)\n\n    if len(embeddings) == 0:\n        return []\n\n    X = np.vstack(embeddings)\n\n    # Heuristic for number of clusters (adjust if you know speaker count)\n    if estimate_speakers is None:\n        # choose between 1 and 6 speakers based on segment count\n        n_clusters = min(6, max(1, int(max(1, round(len(valid_segments) ** 0.5)))))\n        # ensure at least 1\n        n_clusters = max(1, n_clusters)\n    else:\n        n_clusters = int(estimate_speakers)\n\n    # If only 1 segment, label it speaker 0\n    if X.shape[0] == 1:\n        labels = np.array([0])\n    else:\n        # Agglomerative clustering; you can tune linkage / affinity if needed\n        clustering = AgglomerativeClustering(n_clusters=n_clusters)\n        labels = clustering.fit_predict(X)\n\n    # Create enriched segments with speaker labels\n    enriched = []\n    for seg, lab in zip(valid_segments, labels):\n        enriched.append({\n            \"start_time\": round(seg['start'], 2),\n            \"end_time\": round(seg['end'], 2),\n            \"text\": seg['text'],\n            \"speaker\": f\"spk_{int(lab)}\"\n        })\n\n    # Optional: merge adjacent segments with same speaker into longer turns\n    merged = []\n    for seg in enriched:\n        if not merged:\n            merged.append(seg.copy())\n            continue\n        prev = merged[-1]\n        if prev['speaker'] == seg['speaker'] and abs(seg['start_time'] - prev['end_time']) <= 0.5:\n            # extend previous segment\n            prev['end_time'] = seg['end_time']\n            prev['text'] = (prev['text'] + \" \" + seg['text']).strip()\n        else:\n            merged.append(seg.copy())\n    return merged\n\n# Run diarization for all wav files in audio_dir using Whisper segments (all_transcriptions)\nall_transcripts_with_speakers = {}\nfor file in sorted(os.listdir(audio_dir)):\n    if not file.endswith(\".wav\"):\n        continue\n    wav_path = os.path.join(audio_dir, file)\n    base_name = os.path.splitext(file)[0]\n    print(f\"🔊 Diarizing {file} ...\")\n    whisper_segs = all_transcriptions.get(file, [])\n    if not whisper_segs:\n        print(\"  ⚠️ No whisper transcription found for this audio — skipping.\")\n        all_transcripts_with_speakers[base_name] = []\n        continue\n\n    enriched = diarize_file_by_whisper(wav_path, whisper_segs, estimate_speakers=None)\n    all_transcripts_with_speakers[base_name] = enriched\n\n    # print first 5 for quick check\n    for i, line in enumerate(enriched[:5], 1):\n        print(f\"  {i}. [{line['speaker']}] {line['text'][:160]} (Start: {line['start_time']}s, End: {line['end_time']}s)\")\n    print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:30:41.732484Z","iopub.execute_input":"2025-11-26T17:30:41.733268Z","iopub.status.idle":"2025-11-26T17:30:50.592549Z","shell.execute_reply.started":"2025-11-26T17:30:41.733242Z","shell.execute_reply":"2025-11-26T17:30:50.591880Z"}},"outputs":[{"name":"stdout","text":"Loaded the voice encoder model on cuda in 0.02 seconds.\n🔊 Diarizing video_convo.wav ...\n  1. [spk_0]  It's a beautiful house. Do you both live here? Yes. Can I get a short house tour? Yeah, mind your shirt. (Start: 0.0s, End: 5.28s)\n  2. [spk_1] Welcome to our home. Sat Tichit means true consciousness. This is the house which we will  keep in mind that it enhances our consciousness. These are the waste  (Start: 7.12s, End: 18.0s)\n  3. [spk_0]  the colourful reflection on the backside. Oh wow, you have a hand pump at the end. Yeah. (Start: 18.0s, End: 21.92s)\n  4. [spk_0]  We use the rain water to write this. Let's go. (Start: 23.04s, End: 27.36s)\n  5. [spk_3]  It's a tiny bridge. (Start: 27.36s, End: 30.08s)\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# **Step 4: Bucket Transcriptions by Time**\n#### * Organize transcribed text into 5-second time buckets.\n#### * Each segment is assigned to a bucket_start and bucket_end (e.g., 0–5 sec, 5–10 sec).\n#### * Store all segments with their time buckets, speakers, and texts.","metadata":{}},{"cell_type":"code","source":"import math\n\ndef assign_buckets(transcript_segments, bucket_size=5):\n    \"\"\"\n    Adds 5-second time bucket information to each transcription segment.\n\n    Parameters:\n        transcript_segments (list): List of transcription segments with 'start_time' and 'end_time'.\n        bucket_size (int): Size of each time bucket in seconds (default: 5).\n\n    Returns:\n        list of dicts: Updated segments with bucket_start and bucket_end.\n    \"\"\"\n    for segment in transcript_segments:\n        start_bucket = int(math.floor(segment[\"start_time\"] / bucket_size)) * bucket_size\n        end_bucket = start_bucket + bucket_size\n        segment[\"bucket_start\"] = start_bucket\n        segment[\"bucket_end\"] = end_bucket\n    return transcript_segments\n# Dictionary to store the bucketed transcriptions\nbucketed_transcripts = {}\n# Assign buckets to all transcription segments\nfor file_name, segments in all_transcripts_with_speakers.items():\n    print(f\"Assigning buckets for: {file_name}\")\n    \n    # Assign buckets to the segments\n    updated_segments = assign_buckets(segments)\n    bucketed_transcripts[file_name] = updated_segments\n    \n    # Print the first 5 segments \n    print(f\"Buckets assigned. First 5 entries:\")\n    for i, segment in enumerate(updated_segments[:5], start=1):\n        print(f\"{i}. [Speaker: {segment['speaker']}] {segment['text']} (Start: {segment['start_time']}s, End: {segment['end_time']}s) → Bucket: {segment['bucket_start']}-{segment['bucket_end']}s\")\n\n    print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:32:59.110072Z","iopub.execute_input":"2025-11-26T17:32:59.110418Z","iopub.status.idle":"2025-11-26T17:32:59.117004Z","shell.execute_reply.started":"2025-11-26T17:32:59.110395Z","shell.execute_reply":"2025-11-26T17:32:59.116289Z"}},"outputs":[{"name":"stdout","text":"Assigning buckets for: video_convo\nBuckets assigned. First 5 entries:\n1. [Speaker: spk_0]  It's a beautiful house. Do you both live here? Yes. Can I get a short house tour? Yeah, mind your shirt. (Start: 0.0s, End: 5.28s) → Bucket: 0-5s\n2. [Speaker: spk_1] Welcome to our home. Sat Tichit means true consciousness. This is the house which we will  keep in mind that it enhances our consciousness. These are the waste bottles used to build this mud wall, (Start: 7.12s, End: 18.0s) → Bucket: 5-10s\n3. [Speaker: spk_0]  the colourful reflection on the backside. Oh wow, you have a hand pump at the end. Yeah. (Start: 18.0s, End: 21.92s) → Bucket: 15-20s\n4. [Speaker: spk_0]  We use the rain water to write this. Let's go. (Start: 23.04s, End: 27.36s) → Bucket: 20-25s\n5. [Speaker: spk_3]  It's a tiny bridge. (Start: 27.36s, End: 30.08s) → Bucket: 25-30s\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# **Step 5: Sentiment Analysis (Using NLTK VADER)**\n#### * For each transcribed segment, analyze sentiment using the VADER sentiment analyzer.\n#### * Assign a label: positive, negative, or neutral based on compound score.\n#### * Add sentiment to the segment metadata.","metadata":{}},{"cell_type":"code","source":"# Initialize SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\n\ndef analyze_sentiment(text):\n    \"\"\"\n    Analyzes sentiment of a given text using VADER SentimentIntensityAnalyzer.\n\n    Parameters:\n        text (str): The text to analyze.\n\n    Returns:\n        str: Sentiment label: \"positive\", \"negative\", or \"neutral\".\n    \"\"\"\n    # Analyze sentiment\n    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n\n    # Assign sentiment based on the compound score\n    if sentiment_score >= 0.05:\n        return \"positive\"\n    elif sentiment_score <= -0.05:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\ndef apply_sentiment_to_segments(transcript_segments):\n    \"\"\"\n    Applies sentiment analysis to each transcription segment.\n\n    Parameters:\n        transcript_segments (list): List of transcription segments with 'text' and other info.\n\n    Returns:\n        list: Updated transcription segments with sentiment.\n    \"\"\"\n    for segment in transcript_segments:\n        segment[\"sentiment\"] = analyze_sentiment(segment[\"text\"])\n    return transcript_segments\n\n# Dictionary to store sentiment-analyzed transcripts\nsentimented_transcripts = {}\nfor file_name, segments in bucketed_transcripts.items():\n    updated = apply_sentiment_to_segments(segments)\n    sentimented_transcripts[file_name] = updated\n\n# Apply sentiment analysis for all files\nfor file_name, segments in bucketed_transcripts.items():\n    print(f\" Analyzing sentiment for: {file_name}\")\n    \n    # Apply sentiment analysis to segments\n    updated_segments_with_sentiment = apply_sentiment_to_segments(segments)\n    sentimented_transcripts[file_name] = updated_segments_with_sentiment\n    \n    # Print the first 5 segments\n    print(f\"Sentiment analysis complete. First 5 segments:\")\n    \n    for i, entry in enumerate(updated_segments_with_sentiment[:5], start=1):\n        print(f\"{i}. [Speaker: {entry['speaker']}] {entry['text']} (Sentiment: {entry['sentiment']})\")\n        print(f\"   Start Time: {entry['start_time']}s, End Time: {entry['end_time']}s\")\n        print(f\"   Bucket: {entry['bucket_start']} - {entry['bucket_end']}s\")\n    print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:33:04.152215Z","iopub.execute_input":"2025-11-26T17:33:04.152546Z","iopub.status.idle":"2025-11-26T17:33:04.168835Z","shell.execute_reply.started":"2025-11-26T17:33:04.152525Z","shell.execute_reply":"2025-11-26T17:33:04.168074Z"}},"outputs":[{"name":"stdout","text":" Analyzing sentiment for: video_convo\nSentiment analysis complete. First 5 segments:\n1. [Speaker: spk_0]  It's a beautiful house. Do you both live here? Yes. Can I get a short house tour? Yeah, mind your shirt. (Sentiment: positive)\n   Start Time: 0.0s, End Time: 5.28s\n   Bucket: 0 - 5s\n2. [Speaker: spk_1] Welcome to our home. Sat Tichit means true consciousness. This is the house which we will  keep in mind that it enhances our consciousness. These are the waste bottles used to build this mud wall, (Sentiment: positive)\n   Start Time: 7.12s, End Time: 18.0s\n   Bucket: 5 - 10s\n3. [Speaker: spk_0]  the colourful reflection on the backside. Oh wow, you have a hand pump at the end. Yeah. (Sentiment: positive)\n   Start Time: 18.0s, End Time: 21.92s\n   Bucket: 15 - 20s\n4. [Speaker: spk_0]  We use the rain water to write this. Let's go. (Sentiment: neutral)\n   Start Time: 23.04s, End Time: 27.36s\n   Bucket: 20 - 25s\n5. [Speaker: spk_3]  It's a tiny bridge. (Sentiment: neutral)\n   Start Time: 27.36s, End Time: 30.08s\n   Bucket: 25 - 30s\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# **Step 6: Named Entity Recognition (NER)**\n#### * Apply a custom regex-based NER system (no external ML models used).  \n#### * Detect structured entities such as:  \n   - Emails  \n   - URLs  \n   - Dates  \n   - Money amounts  \n   - Phone numbers  \n#### * Identify names and organizations using capitalization-based heuristics.  \n#### * Extract possible locations using preposition + capitalized word patterns.  \n#### * Store all detected entities in a list for each transcription segment.\n","metadata":{}},{"cell_type":"code","source":"\n# === Regex patterns ===\nemail_re = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\", flags=re.I)\nurl_re   = re.compile(r\"(https?://[^\\s,;]+|www\\.[^\\s,;]+)\", flags=re.I)\nmoney_re = re.compile(r\"[₹$€]\\s?\\d+(?:[,\\d]*)?(?:\\.\\d+)?\")\ndate_re  = re.compile(r\"\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+\\d{1,2}(?:,\\s*\\d{4})?)\\b\", flags=re.I)\nphone_re = re.compile(r\"\\+?\\d[\\d\\-\\s()]{6,}\\d\")\n\norg_endings = {\"Ltd\",\"Ltd.\",\"Limited\",\"Inc\",\"Corporation\",\"University\",\"College\",\"Institute\",\"Corp\",\"LLP\",\"PLC\",\"Bank\",\"Foundation\",\"Association\"}\nloc_preps = {\"in\",\"at\",\"from\",\"near\",\"around\",\"to\",\"into\",\"towards\",\"by\"}\n\ndef find_title_sequences(text, min_len=2, max_len=4):\n    tokens = re.findall(r\"\\b[\\w'&\\.-]+\\b\", text)\n    spans = []\n    n = len(tokens)\n    for i in range(n):\n        for L in range(min_len, max_len+1):\n            if i+L <= n:\n                seq = tokens[i:i+L]\n                ok = all([t[0].isupper() for t in seq])\n                if ok:\n                    spans.append(\" \".join(seq))\n    return list(dict.fromkeys(spans))\n\ndef extract_named_entities_no_spacy(text):\n    if not text or not isinstance(text, str):\n        return []\n    ents = []\n\n    # regex entities\n    for reg, lab in [(email_re,\"EMAIL\"),(url_re,\"URL\"),(money_re,\"MONEY\"),\n                     (date_re,\"DATE\"),(phone_re,\"PHONE\")]:\n        for m in reg.finditer(text):\n            tag = f\"{lab}:{m.group(0)}\"\n            if tag not in ents:\n                ents.append(tag)\n\n    # PERSON/ORG heuristics\n    title_seqs = find_title_sequences(text)\n    for seq in title_seqs:\n        if any(tok.rstrip(\".,\") in org_endings for tok in seq.split()):\n            tag = f\"ORG:{seq}\"\n        else:\n            tag = f\"PERSON:{seq}\"\n        if tag not in ents:\n            ents.append(tag)\n\n    # GPE heuristics\n    gpe_matches = re.findall(\n        r\"\\b(\" + \"|\".join(loc_preps) + r\")\\s+([A-Z][\\w\\.-]+(?:\\s+[A-Z][\\w\\.-]+)?)\",\n        text\n    )\n    for m in gpe_matches:\n        city = m[1]\n        tag = f\"GPE:{city}\"\n        if tag not in ents:\n            ents.append(tag)\n\n    return ents\n\ndef apply_ner_to_segments(transcript_segments):\n    for segment in transcript_segments:\n        segment[\"named_entities\"] = extract_named_entities_no_spacy(segment.get(\"text\",\"\"))\n    return transcript_segments\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:35:09.488427Z","iopub.execute_input":"2025-11-26T17:35:09.488745Z","iopub.status.idle":"2025-11-26T17:35:09.500672Z","shell.execute_reply.started":"2025-11-26T17:35:09.488722Z","shell.execute_reply":"2025-11-26T17:35:09.499957Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"nered_transcripts = {}\n\nfor file_name, segments in sentimented_transcripts.items():\n    print(f\"Applying NER for: {file_name}\")\n\n    # Apply NER\n    updated_segments = apply_ner_to_segments(segments)\n    nered_transcripts[file_name] = updated_segments\n\n    # Print first 5 segments like Step 5\n    print(\"First 5 segments with NER:\")\n    for idx, seg in enumerate(updated_segments[:5], start=1):\n        print(f\"{idx}. [Speaker: {seg.get('speaker','')}]\")\n        print(f\"   Text: {seg.get('text','')}\")\n        print(f\"   Named Entities: {seg.get('named_entities', [])}\")\n        print(f\"   Sentiment: {seg.get('sentiment','')}\")\n        print(f\"   Time: {seg.get('start_time')}s → {seg.get('end_time')}s\")\n        print(f\"   Bucket: {seg.get('bucket_start')} - {seg.get('bucket_end')}s\")\n        print(\"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:37:43.420634Z","iopub.execute_input":"2025-11-26T17:37:43.421205Z","iopub.status.idle":"2025-11-26T17:37:43.427628Z","shell.execute_reply.started":"2025-11-26T17:37:43.421180Z","shell.execute_reply":"2025-11-26T17:37:43.426914Z"}},"outputs":[{"name":"stdout","text":"Applying NER for: video_convo\nFirst 5 segments with NER:\n1. [Speaker: spk_0]\n   Text:  It's a beautiful house. Do you both live here? Yes. Can I get a short house tour? Yeah, mind your shirt.\n   Named Entities: ['PERSON:Yes Can', 'PERSON:Yes Can I', 'PERSON:Can I']\n   Sentiment: positive\n   Time: 0.0s → 5.28s\n   Bucket: 0 - 5s\n------------------------------------------------------------\n2. [Speaker: spk_1]\n   Text: Welcome to our home. Sat Tichit means true consciousness. This is the house which we will  keep in mind that it enhances our consciousness. These are the waste bottles used to build this mud wall,\n   Named Entities: ['PERSON:Sat Tichit']\n   Sentiment: positive\n   Time: 7.12s → 18.0s\n   Bucket: 5 - 10s\n------------------------------------------------------------\n3. [Speaker: spk_0]\n   Text:  the colourful reflection on the backside. Oh wow, you have a hand pump at the end. Yeah.\n   Named Entities: []\n   Sentiment: positive\n   Time: 18.0s → 21.92s\n   Bucket: 15 - 20s\n------------------------------------------------------------\n4. [Speaker: spk_0]\n   Text:  We use the rain water to write this. Let's go.\n   Named Entities: []\n   Sentiment: neutral\n   Time: 23.04s → 27.36s\n   Bucket: 20 - 25s\n------------------------------------------------------------\n5. [Speaker: spk_3]\n   Text:  It's a tiny bridge.\n   Named Entities: []\n   Sentiment: neutral\n   Time: 27.36s → 30.08s\n   Bucket: 25 - 30s\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# **Step 7: Export to CSV**\n#### * Create a dedicated output folder (`./csv_outputs`) to store results for each video.  \n#### * Convert each processed segment (containing transcription, speaker, sentiment, entities, and time buckets) into a structured row format.  \n#### * Generate a separate CSV file for every input audio/video file, preserving the filename for traceability.  \n#### * Each CSV includes:  \n   - start_time  \n   - end_time  \n   - bucket_start / bucket_end  \n   - text  \n   - sentiment  \n   - named_entities  \n   - word_count  \n   - speaker  \n#### * Use safe error-handling to skip broken files and continue processing without interruptions.  \n#### * Display a small preview of the first few rows of each generated CSV for verification.  \n","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom pathlib import Path\nimport traceback\n\ndef export_per_video_csvs_safe(nered_transcripts, output_dir=\"./csv_outputs\", show_preview=2):\n    os.makedirs(output_dir, exist_ok=True)\n\n    if not nered_transcripts:\n        print(\"⚠️ nered_transcripts is empty or falsy. Nothing to export.\")\n        return\n\n    exported = 0\n    for file_name, segments in nered_transcripts.items():\n        try:\n            if not segments:\n                print(f\"⚠️ Skipping {file_name}: no segments\")\n                continue\n\n            rows = []\n            for i, segment in enumerate(segments):\n                # defensive getters with defaults\n                start = segment.get(\"start_time\")\n                end   = segment.get(\"end_time\")\n                text  = segment.get(\"text\", \"\")\n                sentiment = segment.get(\"sentiment\", \"\")\n                named_entities = segment.get(\"named_entities\", [])\n                if not isinstance(named_entities, (list, tuple)):\n                    # if it's a string or None, normalize to list\n                    if named_entities:\n                        named_entities = [str(named_entities)]\n                    else:\n                        named_entities = []\n\n                row = {\n                    \"start_time\": start,\n                    \"end_time\": end,\n                    \"bucket_start\": int(start // 5) * 5 if isinstance(start, (int,float)) else None,\n                    \"bucket_end\": (int(start // 5) + 1) * 5 if isinstance(start, (int,float)) else None,\n                    \"text\": text,\n                    \"sentiment\": sentiment,\n                    \"named_entities\": \", \".join(named_entities),\n                    \"word_count\": len(str(text).split()),\n                    \"speaker\": segment.get(\"speaker\", \"\")\n                }\n                rows.append(row)\n\n            if not rows:\n                print(f\"⚠️ Skipping {file_name}: rows empty after processing\")\n                continue\n\n            df = pd.DataFrame(rows)\n            output_path = os.path.join(output_dir, f\"{file_name}.csv\")\n            df.to_csv(output_path, index=False)\n            exported += 1\n\n            print(f\"✅ Exported {file_name} → {output_path} ({len(rows)} rows)\")\n            if show_preview:\n                print(\" Preview rows:\")\n                print(df.head(show_preview).to_dict(orient='records'))\n        except Exception as e:\n            print(f\"❌ Failed exporting {file_name}: {e}\")\n            traceback.print_exc()\n\n    if exported == 0:\n        print(\"⚠️ No CSVs exported. Check that nered_transcripts contains data in expected format.\")\n    else:\n        print(f\"🎉 Exported {exported} CSV file(s) to: {os.path.abspath(output_dir)}\")\n\n# example call\nexport_per_video_csvs_safe(nered_transcripts, output_dir=\"./csv_outputs\", show_preview=2)\n# list files created\nprint(\"Files in output dir:\", list(Path(\"./csv_outputs\").glob(\"*.csv\")))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T17:42:00.175249Z","iopub.execute_input":"2025-11-26T17:42:00.176033Z","iopub.status.idle":"2025-11-26T17:42:00.214937Z","shell.execute_reply.started":"2025-11-26T17:42:00.176006Z","shell.execute_reply":"2025-11-26T17:42:00.214325Z"}},"outputs":[{"name":"stdout","text":"✅ Exported video_convo → ./csv_outputs/video_convo.csv (10 rows)\n Preview rows:\n[{'start_time': 0.0, 'end_time': 5.28, 'bucket_start': 0, 'bucket_end': 5, 'text': \" It's a beautiful house. Do you both live here? Yes. Can I get a short house tour? Yeah, mind your shirt.\", 'sentiment': 'positive', 'named_entities': 'PERSON:Yes Can, PERSON:Yes Can I, PERSON:Can I', 'word_count': 21, 'speaker': 'spk_0'}, {'start_time': 7.12, 'end_time': 18.0, 'bucket_start': 5, 'bucket_end': 10, 'text': 'Welcome to our home. Sat Tichit means true consciousness. This is the house which we will  keep in mind that it enhances our consciousness. These are the waste bottles used to build this mud wall,', 'sentiment': 'positive', 'named_entities': 'PERSON:Sat Tichit', 'word_count': 35, 'speaker': 'spk_1'}]\n🎉 Exported 1 CSV file(s) to: /kaggle/working/csv_outputs\nFiles in output dir: [PosixPath('csv_outputs/video_convo.csv')]\n","output_type":"stream"}],"execution_count":13}]}